%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[
% twocolumn,
% hf,
]{ceurart}

\sloppy

\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{url}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{float}   % [H] placement — keeps figures exactly where declared

\lstset{
  breaklines=true,
  basicstyle=\ttfamily\small
}

% ---------------------------------------------------------------
% Self-contained BibTeX for Overleaf
% ---------------------------------------------------------------
\begin{filecontents*}{references.bib}

% --- Project infrastructure ---
@misc{mii-llm,
  title        = {mii-llm ({M}ade in {I}taly -- {L}arge {L}anguage {M}odel)},
  howpublished = {\url{https://mii-llm.ai}},
  note         = {Accessed 2026-02-18}
}
@misc{seeweb,
  title        = {Seeweb},
  howpublished = {\url{https://www.seeweb.it}},
  note         = {Accessed 2026-02-18}
}

% --- Training frameworks ---
@misc{megatron-lm,
  title        = {Megatron-{LM}},
  howpublished = {\url{https://github.com/NVIDIA/Megatron-LM}},
  note         = {Accessed 2026-02-18}
}
@misc{llama-factory,
  title        = {{LL}a{MA}-Factory},
  howpublished = {\url{https://github.com/hiyouga/LLaMA-Factory}},
  note         = {Accessed 2026-02-18}
}
@misc{nanogpt,
  title        = {nanoGPT},
  howpublished = {\url{https://github.com/karpathy/nanoGPT}},
  note         = {Accessed 2026-02-18}
}
@misc{nanochat,
  title        = {nanochat},
  howpublished = {\url{https://github.com/karpathy/nanochat}},
  note         = {Accessed 2026-02-18}
}
@misc{nanotron,
  title        = {Hugging {F}ace Nanotron},
  howpublished = {\url{https://github.com/huggingface/nanotron}},
  note         = {Accessed 2026-02-18}
}
@misc{nanotron-fork,
  title        = {mii-llm Nanotron Fork},
  howpublished = {\url{https://github.com/mii-llm/nanotron}},
  note         = {Accessed 2026-02-18}
}

% --- Data ---
@misc{datatrove,
  title        = {datatrove},
  howpublished = {\url{https://github.com/huggingface/datatrove}},
  note         = {Accessed 2026-02-18}
}
@misc{fineweb,
  title        = {{HuggingFaceFW/fineweb} (sample-350{BT})},
  howpublished = {\url{https://huggingface.co/datasets/HuggingFaceFW/fineweb/viewer/sample-350BT}},
  note         = {Accessed 2026-02-18}
}
@misc{fineweb2,
  title        = {{HuggingFaceFW/fineweb-2}},
  howpublished = {\url{https://huggingface.co/datasets/HuggingFaceFW/fineweb-2}},
  note         = {Accessed 2026-02-18}
}
@misc{finepdfs,
  title        = {{HuggingFaceFW/finepdfs}},
  howpublished = {\url{https://huggingface.co/datasets/HuggingFaceFW/finepdfs}},
  note         = {Accessed 2026-02-18}
}
@misc{starcoderdata,
  title        = {bigcode/starcoderdata},
  howpublished = {\url{https://huggingface.co/datasets/bigcode/starcoderdata}},
  note         = {Accessed 2026-02-18}
}
@misc{openitaliandata,
  title        = {{DeepMount00/OpenItalianData}},
  howpublished = {\url{https://huggingface.co/datasets/DeepMount00/OpenItalianData}},
  note         = {Accessed 2026-02-18}
}

% --- Evaluation harnesses ---
@misc{lm-eval-harness,
  title        = {{EleutherAI} lm-evaluation-harness},
  howpublished = {\url{https://github.com/EleutherAI/lm-evaluation-harness}},
  note         = {Accessed 2026-02-18}
}
@misc{eduagarcia-lmevalpt,
  title        = {eduagarcia/lm-evaluation-harness-pt},
  howpublished = {\url{https://github.com/eduagarcia/lm-evaluation-harness-pt}},
  note         = {Accessed 2026-02-18}
}
@misc{openptleaderboard,
  title        = {Open {P}ortuguese {LLM} {L}eaderboard},
  howpublished = {\url{https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard}},
  note         = {Accessed 2026-02-18}
}

% --- Benchmark papers ---
@article{hendrycks2021mmlu,
  title   = {Measuring {M}assive {M}ultitask {L}anguage {U}nderstanding},
  author  = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou
             and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal = {Proceedings of the International Conference on Learning
             Representations ({ICLR})},
  year    = {2021}
}
@inproceedings{zellers2019hellaswag,
  title     = {{H}ella{S}wag: {C}an a {M}achine {R}eally {F}inish {Y}our {S}entence?},
  author    = {Rowan Zellers and Ari Holtzman and Yonatan Bisk
               and Ali Farhadi and Yejin Choi},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association
               for Computational Linguistics ({ACL})},
  year      = {2019}
}
@article{clark2018arc,
  title   = {Think You Have Solved Question Answering? {T}ry {ARC},
             the {AI2} {R}easoning {C}hallenge},
  author  = {Peter Clark and Isaac Cowhey and Oren Etzioni and Tushar Khot
             and Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  journal = {arXiv preprint arXiv:1803.05457},
  year    = {2018}
}
@misc{zhou2023ifeval,
  title         = {Instruction-Following Evaluation for Large Language Models},
  author        = {Jeffrey Zhou and Tianjian Lu and Swaroop Mishra and
                   Siddhartha Brahma and Sujoy Basu and Yi Luan and
                   Denny Zhou and Le Hou},
  year          = {2023},
  eprint        = {2311.07911},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

% --- Post-training methods ---
@inproceedings{ouyang2022instructgpt,
  title     = {Training Language Models to Follow Instructions with Human Feedback},
  author    = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and
               Carroll L. Wainwright and Pamela Mishkin and Chong Zhang and
               Sandhini Agarwal and Katarina Slama and Alex Ray and
               John Schulman and Jacob Hilton and Fraser Kelton and
               Luke E. Miller and Maddie Simens and Amanda Askell and
               Peter Welinder and Paul F. Christiano and Jan Leike and
               Ryan J. Lowe},
  booktitle = {Advances in Neural Information Processing Systems ({NeurIPS})},
  year      = {2022}
}
@inproceedings{rafailov2023dpo,
  title     = {Direct {P}reference {O}ptimization: {Y}our {L}anguage {M}odel is
               {S}ecretly a {R}eward {M}odel},
  author    = {Rafael Rafailov and Archit Sharma and Eric Mitchell and
               Christopher D. Manning and Stefano Ermon and Chelsea Finn},
  booktitle = {Advances in Neural Information Processing Systems ({NeurIPS})},
  year      = {2023}
}

% --- Italian NLP ---
@inproceedings{evalita2023,
  title     = {{EVALITA} 2023: {O}verview of the 8th {E}valuation {C}ampaign of
               {N}atural {L}anguage {P}rocessing and {S}peech {T}ools for {I}talian},
  author    = {Mirko Lai and Stefano Menini and Marco Polignano and
               Valentina Russo and Rachele Sprugnoli and Giulia Venturi},
  booktitle = {Proceedings of the Eighth Evaluation Campaign of {NLP} and
               Speech Tools for Italian ({EVALITA} 2023)},
  year      = {2023},
  publisher = {CEUR.org}
}
\end{filecontents*}
% ---------------------------------------------------------------

\begin{document}

\copyrightyear{2026}
\copyrightclause{Copyright for this paper by its authors.
  Use permitted under Creative Commons License Attribution 4.0
  International (CC BY 4.0).}

\conference{Twelfth Italian Conference on Computational Linguistics (CLiC-it 2026),
  September 14--16, 2026, Palermo, Italy}

\title{The Joy and Pain of Training an LLM from Scratch}
\subtitle{A Technical Report on the Development of the Zagreus and Nesso Model Families}

\author[1]{Anonymous Author}
\address[1]{Institution, Country}

\begin{abstract}
Training a fully functional modern neural network, specifically a Large Language Model
(LLM), from first principles has been a foundational ambition since the inception of our
community mii-llm (Made in Italy -- Large Language Model). This technical report documents
the development of the Zagreus and Nesso model families, spanning motivation, framework
selection, data engineering, pre-training, post-training, and a comprehensive set of
reproducible evaluations across multiple languages.
\end{abstract}

\begin{keywords}
Small Language Models \sep
European Languages \sep
Pre-training \sep
Post-training \sep
Evaluation \sep
Reproducibility
\end{keywords}

\maketitle

% LaTeX generates the ToC automatically — no manual \section needed.
\tableofcontents

% ================================================================
\section{Motivation: The Vision of Sovereign Edge Intelligence}
\label{sec:motivation}
% ================================================================

Training a fully functional modern neural network, specifically a Large Language Model
(LLM), from first principles has been a foundational ambition since the inception of our
community \href{https://mii-llm.ai}{mii-llm}~\cite{mii-llm} that stands for Made in Italy
-- Large Language Model.

In the current landscape, the convergence of distributed computing power and accessible
knowledge has never been more potent; consequently, constructing an intelligent machine
stands as one of the most exciting tasks a group of machine learning specialists can
undertake. This vision materialized when Antonio Baldassarra (CEO of Seeweb) and Marco
Cristofanilli (Head of AI at Seeweb) commissioned us to develop a Small Language Model
(SLM) from scratch utilizing the Seeweb infrastructure~\cite{seeweb}.

Seeweb, a cloud provider with a strategic focus on AI, granted us access to a cluster of
on-demand nodes comprising a total of 64 NVIDIA A100 GPUs. Our primary objective was to
experiment and deliver a state-of-the-art SLM with approximately 500 million parameters,
built from the ground up and optimized for edge use cases within some of the romance
languages ecosystem, in particular Italian, Spanish, Portuguese and French.

We hypothesize that, in the coming years, intelligent devices---and virtually any hardware
equipped with a chip---will be enhanced by neural architectures with embedded reasoning and
language capabilities. Small, efficient models will be key to enabling automation at the
edge. To address this need, we created four foundation language models, the Zagreus family,
and three different finetuned models, the Nesso family, arguably one of the few
high-performing small language models dedicated to the European languages.

In the spirit of open and reproducible research, we are releasing the full Zagreus and
Nesso lineup: seven models in total: four base (pretrained) checkpoints for bilingual
models and three post-trained variants. Notably, our post-trained models are designed to
compete on standard benchmarks with state of the art models of comparable size,
demonstrating that carefully engineered small models can achieve near frontier-level
performance within their parameter class.

\subsection*{Base models}
\begin{itemize}
  \item \texttt{zagreus-0.4B-base-ita}: English--Italian bilingual model
  \item \texttt{zagreus-0.4B-base-spa}: English--Spanish bilingual model
  \item \texttt{zagreus-0.4B-base-por}: English--Portuguese bilingual model
  \item \texttt{zagreus-0.4B-base-fra}: English--French bilingual model
\end{itemize}

\subsection*{Post-trained models}
\begin{itemize}
  \item \texttt{Nesso-0.4B-instruct}: English--Italian for conversational use cases
  \item \texttt{Nesso-0.4B-agentic}: English--Italian for agentic and function-calling use cases
  \item \texttt{Open-Zagreus-0.4B}: Fully open-source data used to train this model
\end{itemize}

We are releasing this detailed technical report, covering every step and data point required
to reproduce the project, as we strongly believe in the importance of open source in
reducing technological and geopolitical dependencies.

% ================================================================
\section{Technology Stack: Framework Selection}
\label{sec:framework}
% ================================================================

There are numerous frameworks available for creating an LLM from scratch. We conducted a
comparative analysis of several options. Below is a summary of our testing and the
rationale behind our ultimate decision to utilize Nanotron by Hugging Face~\cite{nanotron}.

\subsection{Framework Comparative Analysis}

\paragraph{Megatron-LM.}
Developed by NVIDIA, this is a powerful framework designed for training large transformer
models with billions of parameters~\cite{megatron-lm}. While it is likely an optimal
choice for large, well-resourced teams, we found it challenging to set up and deploy
effectively on our specific cluster infrastructure.

\paragraph{Llama-Factory.}
A versatile and user-friendly open-source framework that simplifies fine-tuning, training,
and deployment of a wide range of LLMs~\cite{llama-factory}. However, our evaluation
suggests it is more specialized for fine-tuning than for pre-training from scratch.

\paragraph{nanoGPT and nanochat.}
Both created by Andrej Karpathy, these projects prioritize simplicity and educational value.
\href{https://github.com/karpathy/nanoGPT}{nanoGPT}~\cite{nanogpt} is a minimalist,
readable codebase designed as a learning tool, though it is now considered deprecated in
favor of its successor.
\href{https://github.com/karpathy/nanochat}{nanochat}~\cite{nanochat} is the evolution of
nanoGPT, offering a full-stack, end-to-end pipeline for building a complete ChatGPT-like
chatbot. It covers the entire lifecycle, from tokenization and pre-training to fine-tuning
and a web interface, all within a compact and hackable codebase. Although nanochat had not
yet been released when we commenced this project, we believe it has a promising future,
especially given its recent integration into the Transformers library.

\subsection{Our Choice: Hugging Face Nanotron}

Ultimately, we selected
\href{https://github.com/huggingface/nanotron}{Hugging Face Nanotron}~\cite{nanotron}. It
is a minimalistic library focused on 3D parallelism (Data, Tensor, and Pipeline)
specifically for pre-training transformer models. We value Hugging Face for its commitment
to openness. We found the library well-suited for multi-node training; furthermore, it is
natively integrated into the Hugging Face ecosystem (Accelerate, Datasets, hf-cli),
ensuring that workflows---from data tokenization to model release---remain cohesive. During
the development cycle, we identified minor bugs and are actively contributing to the library
via Pull Requests. We also established a
\href{https://github.com/mii-llm/nanotron}{fork of Nanotron}~\cite{nanotron-fork}
optimized to run directly on a Slurm cluster.

% ================================================================
\section{Data Engineering: The Tokenization Pipeline}
\label{sec:data}
% ================================================================

Data is the \emph{sine qua non} for creating an LLM. The volume of data required is
contingent upon the target model size and the available compute budget. Operating as a
GPU-constrained team---and thanks to the sponsorship from Seeweb---we chose to build a
small language model of \(\sim\)500 million parameters, trained on approximately 1 trillion
tokens.

\subsection{Dataset Sources}

We utilized exclusively open-source datasets by the Hugging Face team for creating our four
bilingual foundational models. Below is the data distribution per model:

\paragraph{mii-llm/nesso-0.4B-ita:}
\begin{itemize}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb/viewer/sample-350BT}~\cite{fineweb}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb-2/viewer/ita_Latn}~\cite{fineweb2}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/finepdfs/viewer/ita_Latn}~\cite{finepdfs}
  \item \url{https://huggingface.co/datasets/bigcode/starcoderdata}~\cite{starcoderdata}
\end{itemize}

\paragraph{mii-llm/nesso-0.4B-fra:}
\begin{itemize}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb/viewer/sample-350BT}~\cite{fineweb}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb-2/viewer/fra_Latn}~\cite{fineweb2}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/finepdfs/viewer/fra_Latn}~\cite{finepdfs}
  \item \url{https://huggingface.co/datasets/bigcode/starcoderdata}~\cite{starcoderdata}
\end{itemize}

\paragraph{mii-llm/nesso-0.4B-por:}
\begin{itemize}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb/viewer/sample-350BT}~\cite{fineweb}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb-2/viewer/por_Latn}~\cite{fineweb2}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/finepdfs/viewer/por_Latn}~\cite{finepdfs}
  \item \url{https://huggingface.co/datasets/bigcode/starcoderdata}~\cite{starcoderdata}
\end{itemize}

\paragraph{mii-llm/nesso-0.4B-spa:}
\begin{itemize}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb/viewer/sample-350BT}~\cite{fineweb}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/fineweb-2/viewer/spa_Latn}~\cite{fineweb2}
  \item \url{https://huggingface.co/datasets/HuggingFaceFW/finepdfs/viewer/spa_Latn}~\cite{finepdfs}
  \item \url{https://huggingface.co/datasets/bigcode/starcoderdata}~\cite{starcoderdata}
\end{itemize}

\subsection{The Tokenization Process}

Raw datasets are not ready for immediate training; they must first be tokenized.
Tokenization is a CPU-intensive process that transforms text strings into token sequences
(numerical IDs). As a rule of thumb for storage estimation, for every 1~GB of text,
approximately 3~GB of tokenized outputs are generated. For \(\sim\)1 trillion tokens, one
typically requires at least 3 to 5 terabytes of disk space (depending on format, sharding
strategy, and compression).

We selected the Llama-3.2 tokenizer (from the Llama-3.2-1B model) because its multilingual
tokenization capabilities are robust and widely adopted. Using the
\href{https://github.com/huggingface/datatrove}{datatrove} library~\cite{datatrove}, the
process took over three weeks of continuous computation to generate \(\sim\)1 trillion
tokens, stratified as roughly 400B English, 400B Italian, and 200B Code.

% ================================================================
\section{Pre-training: The Core Engine}
\label{sec:pretraining}
% ================================================================

Pre-training is the foundational step in building an LLM, transforming raw tokenized data
into a model capable of context-aware text completion. This is the most time-consuming and
GPU-intensive phase. While massive models may require thousands of GPUs, our sub-1-billion
parameter model was effectively trained on the 64 GPU cluster provided by
Seeweb~\cite{seeweb}.

We utilized Nanotron~\cite{nanotron}, which supports multiple architectures, including
Llama-3.2, Qwen-2.5, and Mixture-of-Experts (MoE) variants. For this project, we adopted
a modified Llama-3.2 fully dense architecture. Our design choice was motivated by the
hypothesis that, in the small-parameter regime (\(\sim\)500M parameters), fully dense
models provide better compute utilization and more stable training dynamics than sparse
architectures such as MoE. In tightly constrained capacity settings, the routing overhead
and expert under-utilization typical of MoE architectures may offset their theoretical
efficiency advantages.

Working with a GPU cluster is streamlined by HPC tools; we employed the Slurm scheduler.
Slurm allows the cluster to be viewed as a unified Linux system where jobs can be executed
across many GPUs in parallel, while handling checkpoints and logs in real time. The most
challenging aspect remains ensuring the software stack---from drivers and CUDA/NCCL to
Python libraries---functions harmoniously, often requiring resolution of version and ABI
incompatibilities.

Successfully running a distributed training job on the tokenized data was a profound
milestone. Observing the loss curve decrease from raw data after days of waiting conveys
the sense of operating at the edge of scientific and engineering capability---a genuinely
intense moment for a researcher.

For out-of-the-box functionality, we recommend our fork:
\url{https://github.com/mii-llm/nanotron}~\cite{nanotron-fork} (a fork of
\url{https://github.com/huggingface/nanotron/}~\cite{nanotron}), pending the merge of our
Pull Request.

% ================================================================
\section{Post-Training: Shaping Behavior}
\label{sec:posttraining}
% ================================================================

Creating a base model from scratch represents a major technical achievement, and we
consider this work a contribution to the open community. However, a foundation model
alone---even with a fully reproducible pipeline and transparent data distribution---is
rarely sufficient for direct real-world deployment. The post-training
phase~\cite{ouyang2022instructgpt} is responsible for shaping the model's behavior toward
practical usability.

This phase typically requires significantly fewer GPUs and a smaller data volume compared
to pre-training. However, the \emph{quality} and \emph{curation strategy} of the data
become substantially more important than raw scale.

We utilized Axolotl for post-training due to our extensive experience with the framework
and its stability in multi-GPU environments. While we initially encountered configuration
challenges when integrating it with our Slurm-based HPC setup, we successfully adapted the
workflow to support distributed execution.

We possess extensive experience in post-training language models. Over the past several
years, we have post-trained models for domain-specific applications including finance,
cybersecurity, structured function calling, and agentic execution patterns. Through this
work, we have curated a substantial internal dataset collection that enables controlled
experimentation across varied instruction-following regimes. This dataset collection, built
with meticulous care and long-term iteration, constitutes a strategic asset for our research
group. For this reason, we have decided not to publish it as open source, as we consider it
a competitive advantage. Nevertheless, we believe that releasing the trained models and all
evaluation results provides significant value to the broader community. Most importantly, we
demonstrate that we have been able to build and release a model that performs competitively
head-to-head with state-of-the-art models of similar parameter scale.

We are releasing three primary post-trained models:
\begin{itemize}
  \item \textbf{Nesso-0.4B-instruct}: optimized for conversational and
        instruction-following use cases.
  \item \textbf{Nesso-0.4B-agentic}: optimized for function calling, structured outputs,
        and agentic execution patterns.
\end{itemize}

Both models utilize \textbf{Nesso-0.4B-ita} as the base and are trained on a bilingual
corpus (English/Italian). It is important to note that both models are currently at the
\textbf{SFT (Supervised Fine-Tuning)} stage~\cite{ouyang2022instructgpt}. In the coming
weeks, we will execute the \textbf{DPO (Direct Preference Optimization)}~\cite{rafailov2023dpo}
stage and subsequently update both the models and their evaluation results.

We also released a third, fully open model: \textbf{Open-Zagreus-0.4B}. Thanks to the work
of the Italian open-source community mii-llm~\cite{mii-llm}, and in particular Michele
Montebovi who published the SFT dataset
\emph{OpenItalianData}~\cite{openitaliandata}---all data used and all training recipes for
this model are fully open and reproducible as a full open-source model from data to
weights.

% ================================================================
\section{Pre-trained Foundational Models Evaluations}
\label{sec:base-eval}
% ================================================================

This section presents quantitative evaluations of our pre-trained foundational models. We
include multiple data points to demonstrate how our data curation strategy and
architectural configuration enabled the training of competitive small language model
families. These results serve both as validation and as a reproducible baseline for future
experiments.

We are contributors to lm-evaluation-harness~\cite{lm-eval-harness} for multilingual
benchmarks and relied extensively on this framework. Evaluations cover three standard
benchmarks: MMLU~\cite{hendrycks2021mmlu} (5-shot accuracy),
HellaSwag~\cite{zellers2019hellaswag} (normalized accuracy), and
ARC~\cite{clark2018arc} (normalized accuracy). For each benchmark, we provide the exact
command used to ensure reproducibility.

% ----------------------------------------------------------------
\subsection{Zagreus-0.4B-ita-base}
\label{subsec:zagreus-ita}
% ----------------------------------------------------------------

\subsubsection*{Evaluation Command}
\begin{lstlisting}[language=bash]
lm-eval --model hf --model_args pretrained=checkpoint \
  --tasks m_mmlu_it --num_fewshot 5 --device cuda:0 --batch_size 1

lm-eval --model hf --model_args pretrained=LiquidAI/LFM2-350M \
  --tasks hellaswag_it,arc_it --device cuda:0 --batch_size 1
\end{lstlisting}

\subsubsection*{Checkpoint progression}

Table~\ref{tab:zagreus-ita} shows how MMLU~\cite{hendrycks2021mmlu},
HellaSwag~\cite{zellers2019hellaswag}, and ARC~\cite{clark2018arc} scores evolve across
training checkpoints. Figure~\ref{fig:zagreus-ita} plots the same data.

\begin{table*}[H]
\caption{Checkpoint progression for Zagreus-0.4B-ita-base. Metrics: MMLU (acc),
         HellaSwag (acc\_norm), ARC (acc\_norm).}
\label{tab:zagreus-ita}
\centering
\begin{tabular}{lcccc}
\toprule
Checkpoint & mmlu\_it (acc) & hellaswag\_it (acc\_norm) & arc\_it (acc\_norm) & Media \\
\midrule
v2-95k  & 0.2529 & 0.3366 & 0.2652 & 0.2849 \\
v2-205k & 0.2628 & ---    & ---    & 0.2628 \\
v2-290k & 0.2428 & 0.3492 & 0.2335 & 0.2752 \\
v2-305k & 0.2598 & 0.3562 & 0.2652 & 0.2937 \\
v2-365k & 0.2566 & 0.3664 & 0.2712 & 0.2981 \\
v2-390k & 0.2556 & 0.3438 & 0.2498 & 0.2831 \\
v2-460k & 0.2540 & 0.3778 & 0.2549 & 0.2956 \\
v2-520k & 0.2540 & 0.3778 & 0.2549 & 0.2956 \\
v2-590k & 0.2547 & 0.3651 & 0.2455 & 0.2884 \\
v2-630k & 0.2562 & 0.3632 & 0.2643 & 0.2946 \\
v2-680k & 0.2538 & 0.3740 & 0.2592 & 0.2957 \\
v2-775k & 0.2535 & 0.3750 & 0.2583 & 0.2956 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/zagreus-ita.png}
\caption{Score progression across checkpoints for Zagreus-0.4B-ita-base (data from
         Table~\ref{tab:zagreus-ita}).}
\label{fig:zagreus-ita}
\end{figure}

% ................................................................
\subsection{Evalita Evaluation from FBK (Zagreus-0.4B-ita-base)}
\label{subsec:evalita-fbk}
% ................................................................

In addition to the standard multilingual benchmarks, we evaluate the Italian base model on
the EVALITA benchmark suite~\cite{evalita2023} provided by FBK.

\subsubsection*{Evaluation Command}
\begin{lstlisting}[language=bash]
lm_eval --model hf \
  --model_args pretrained=meta-llama/Llama-2-7b-hf \
  --tasks evalita-mp --device cuda:0 --batch_size 1
\end{lstlisting}

Table~\ref{tab:evalita-fbk} reports per-task results on
EVALITA~\cite{evalita2023}.

\begin{table}[H]
\caption{EVALITA evaluation from FBK --- Zagreus-0.4B-ita-base, per-task
         breakdown~\cite{evalita2023}.}
\label{tab:evalita-fbk}
\centering
\begin{tabular}{l l c}
\toprule
Task & Metric & Value \\
\midrule
\textbf{Evalita-LLM (overall)} & acc    & \textbf{0.3226} \\
admission-test                & acc    & 0.2137 \\
faq                           & acc    & 0.2681 \\
hate-speech-detection         & f1     & 0.6056 \\
lexical-substitution          & f1     & 0.0000 \\
evalita NER                   & f1     & 0.1611 \\
relation-extraction           & f1     & 0.1244 \\
sentiment-analysis            & f1     & 0.3660 \\
summarization-fanpage         & rouge1 & 0.1947 \\
text-entailment               & acc    & 0.5133 \\
word-in-context               & f1     & 0.4697 \\
\bottomrule
\end{tabular}
\end{table}

% ----------------------------------------------------------------
\subsection{Zagreus-0.4B-spa-base (Spanish)}
\label{subsec:zagreus-spa}
% ----------------------------------------------------------------

\subsubsection*{Evaluation Command}
\begin{lstlisting}[language=bash]
lm-eval --model hf --model_args pretrained=checkpoint \
  --tasks m_mmlu_es --num_fewshot 5 --device cuda:0 --batch_size 1

lm-eval --model hf --model_args pretrained=LiquidAI/LFM2-350M \
  --tasks hellaswag_es,arc_es --device cuda:0 --batch_size 1
\end{lstlisting}

Table~\ref{tab:zagreus-spa} and Figure~\ref{fig:zagreus-spa} show results across training
steps.

\begin{table}[H]
\caption{Zagreus-0.4B-spa-base results across training steps.}
\label{tab:zagreus-spa}
\centering
\begin{tabular}{lcccc}
\toprule
Steps & mmlu\_es & arc\_es & hellaswag\_es & Average \\
\midrule
146k & 0.254 & 0.265 & 0.409 & 0.309 \\
216k & 0.237 & 0.270 & 0.414 & 0.307 \\
292k & 0.254 & 0.262 & 0.417 & 0.311 \\
406k & 0.254 & 0.269 & 0.423 & 0.315 \\
518k & 0.255 & 0.280 & 0.429 & 0.321 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/zagreus-spa.png}
\caption{Score progression across training steps for Zagreus-0.4B-spa-base (data from
         Table~\ref{tab:zagreus-spa}).}
\label{fig:zagreus-spa}
\end{figure}

% ----------------------------------------------------------------
\subsection{Zagreus-0.4B-fra (French)}
\label{subsec:zagreus-fra}
% ----------------------------------------------------------------

\subsubsection*{Evaluation Command}
\begin{lstlisting}[language=bash]
lm-eval --model hf --model_args pretrained=checkpoint \
  --tasks m_mmlu_fr --num_fewshot 5 --device cuda:0 --batch_size 1

lm-eval --model hf --model_args pretrained=LiquidAI/LFM2-350M \
  --tasks hellaswag_fr,arc_fr --device cuda:0 --batch_size 1
\end{lstlisting}

Evaluation procedure identical to previous sections. Table~\ref{tab:zagreus-fra} and
Figure~\ref{fig:zagreus-fra} report the results.

\begin{table}[H]
\caption{Zagreus-0.4B-fra results across training steps.}
\label{tab:zagreus-fra}
\centering
\begin{tabular}{lcccc}
\toprule
Steps & m\_mmlu\_fr & arc\_fr & hellaswag\_fr & Average \\
\midrule
129k & 0.262 & ---   & ---   & 0.262 \\
231k & 0.263 & ---   & ---   & 0.263 \\
365k & 0.256 & 0.278 & 0.414 & 0.316 \\
456k & 0.267 & ---   & ---   & 0.267 \\
603k & 0.256 & 0.278 & 0.414 & 0.316 \\
705k & 0.266 & 0.281 & 0.417 & 0.321 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/zagreus-fra.png}
\caption{Score progression across training steps for Zagreus-0.4B-fra (data from
         Table~\ref{tab:zagreus-fra}).}
\label{fig:zagreus-fra}
\end{figure}

% ----------------------------------------------------------------
\subsection{Zagreus-0.4B-por (Portuguese)}
\label{subsec:zagreus-por}
% ----------------------------------------------------------------

\subsubsection*{Evaluation Command}
\begin{lstlisting}[language=bash]
lm-eval --model hf --model_args pretrained=checkpoint \
  --tasks m_mmlu_pt --num_fewshot 5 --device cuda:0 --batch_size 1

lm-eval --model hf --model_args pretrained=LiquidAI/LFM2-350M \
  --tasks hellaswag_pt,arc_pt --device cuda:0 --batch_size 1
\end{lstlisting}

Table~\ref{tab:zagreus-por} and Figure~\ref{fig:zagreus-por} report the results.

\begin{table}[H]
\caption{Zagreus-0.4B-por results across checkpoints.}
\label{tab:zagreus-por}
\centering
\begin{tabular}{lcccc}
\toprule
Checkpoint & ARC & HellaSwag & MMLU & Media \\
\midrule
153k & 0.2667 & 0.3732 & 0.2685 & 0.3028 \\
207k & 0.2705 & 0.3768 & 0.2671 & 0.3048 \\
276k & 0.2718 & 0.3789 & 0.2664 & 0.3057 \\
345k & 0.2564 & 0.3796 & 0.2669 & 0.3009 \\
414k & 0.2682 & 0.3842 & 0.2673 & 0.3066 \\
483k & 0.2667 & 0.3865 & 0.2658 & 0.3063 \\
582k & 0.2786 & 0.3865 & 0.2688 & 0.3113 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/zagreus-por.png}
\caption{Score progression across checkpoints for Zagreus-0.4B-por (data from
         Table~\ref{tab:zagreus-por}).}
\label{fig:zagreus-por}
\end{figure}

% ----------------------------------------------------------------
\subsection{Portuguese Evaluation: lm-evaluation-harness-pt}
\label{subsec:por-garcia}
% ----------------------------------------------------------------

For the Portuguese base model, we also evaluate against the work of Eduardo Garcia and his
fork of lm-eval~\cite{eduagarcia-lmevalpt}, which includes an important open
leaderboard~\cite{openptleaderboard} comparing many open-source models. Table~\ref{tab:por-garcia}
and Figure~\ref{fig:por-garcia} report our results and the comparison with Qwen3-0.6B-Base.

\subsubsection*{Evaluation Command}
\begin{lstlisting}[language=bash]
lm_eval --model huggingface \
  --model_args "pretrained=giux78/zagreus-3B-165000,revision=main" \
  --tasks enem_challenge,bluex,oab_exams,assin2_rte,assin2_sts,\
faquad_nli,hatebr_offensive,portuguese_hate_speech,tweetsentbr \
  --device cuda:0 --output_path "./"
\end{lstlisting}

\begin{table*}[H]
\caption{Portuguese leaderboard-style evaluation
         (lm-evaluation-harness-pt~\cite{eduagarcia-lmevalpt}).
         Zagreus 483k ranks first overall.}
\label{tab:por-garcia}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccccccccc}
\toprule
Model / Checkpoint & RTE & STS & BLUEX & ENEM & FAQUAD NLI & HateBR & OAB
  & PT Hate & TweetSent & \textbf{Media} \\
\midrule
zagreus 483k    & 0.4624 & 0.1650 & 0.2434 & 0.2071 & 0.4397
  & 0.3327 & 0.2528 & 0.4817 & 0.3220 & \textbf{0.3230} \\
zagreus 582k    & 0.3361 & 0.0449 & 0.2100 & 0.1903 & 0.4397
  & 0.3825 & 0.2392 & 0.4444 & 0.1542 & \textbf{0.2713} \\
Qwen3-0.6B-Base & 0.3333 & 0.0726 & 0.1057 & 0.0077 & 0.4397
  & 0.3333 & 0.0428 & 0.4123 & 0.5646 & \textbf{0.2569} \\
\bottomrule
\end{tabular}%
}
\end{table*}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/por-garcia.png}
\caption{Portuguese leaderboard comparison between Zagreus checkpoints and Qwen3-0.6B-Base
         (data from Table~\ref{tab:por-garcia}).}
\label{fig:por-garcia}
\end{figure}

% ================================================================
\section{Post-Trained Nesso Models Evaluations}
\label{sec:nesso-eval}
% ================================================================

In this section, we analyze the performance of \textbf{Nesso-0.4B-instruct} and
\textbf{Nesso-0.4B-agentic} relative to comparable models. Since these models are
pre-trained in English--Italian, we evaluate them on both English and Italian benchmarks:
MMLU~\cite{hendrycks2021mmlu}, HellaSwag~\cite{zellers2019hellaswag},
ARC~\cite{clark2018arc}, and IFEval~\cite{zhou2023ifeval}.

\subsubsection*{Evaluation Commands}
\begin{lstlisting}[language=bash]
lm-eval --model hf --model_args pretrained=checkpoint \
  --tasks m_mmlu_it --num_fewshot 5 --device cuda:0 --batch_size 1

lm-eval --model hf --model_args pretrained=checkpoint \
  --tasks mmlu --num_fewshot 5 --device cuda:0 --batch_size 1

lm-eval --model hf --model_args pretrained=LiquidAI/LFM2-350M \
  --tasks hellaswag_it,arc_it --device cuda:0 --batch_size 1

lm-eval --model hf --model_args pretrained=LiquidAI/LFM2-350M \
  --tasks hellaswag,arc --device cuda:0 --batch_size 1

lm-eval --model hf --model_args pretrained=LiquidAI/LFM2-350M \
  --tasks ifeval-ita --device cuda:0 --batch_size 1

lm-eval --model hf --model_args pretrained=LiquidAI/LFM2-350M \
  --tasks ifeval --device cuda:0 --batch_size 1
\end{lstlisting}

Table~\ref{tab:nesso-main} compares all four models across both languages. Aggregated and
per-language charts are shown in Figures~\ref{fig:nesso-all},~\ref{fig:italian-nesso},
and~\ref{fig:english-nesso}.

\begin{table*}[H]
\caption{English/Italian benchmark comparison across post-trained and baseline models.
         Benchmarks: IFEval~\cite{zhou2023ifeval}, ARC~\cite{clark2018arc},
         HellaSwag~(HS)~\cite{zellers2019hellaswag}, MMLU~\cite{hendrycks2021mmlu}.}
\label{tab:nesso-main}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccccccccccc}
\toprule
Model & IFEval EN & ARC\_EN & HS\_EN & MMLU\_EN & Media EN
      & IFEval IT & ARC\_IT & HS\_IT & MMLU\_IT & Media IT & Media Totale \\
\midrule
Qwen/Qwen3-0.6B    & 0.2758 & 0.3430 & 0.4742 & 0.4013 & 0.3736
                   & 0.3058 & 0.2729 & 0.3598 & 0.4025 & 0.3353 & 0.3545 \\
Nesso-04B-instruct & 0.3465 & 0.3003 & 0.4629 & 0.2871 & 0.3492
                   & 0.2962 & 0.2874 & 0.4076 & 0.2875 & 0.3197 & 0.3345 \\
Nesso-04B-agentic  & 0.2962 & 0.2534 & 0.4062 & 0.2889 & 0.3112
                   & 0.2914 & 0.2541 & 0.3673 & 0.2730 & 0.2965 & 0.3039 \\
LiquidAI/LFM2-350M & 0.1595 & 0.2457 & 0.3092 & 0.3445 & 0.2647
                   & 0.1427 & 0.2464 & 0.2994 & 0.3132 & 0.2504 & 0.2576 \\
\bottomrule
\end{tabular}%
}
\end{table*}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/nesso-all.png}
\caption{Overall benchmark comparison (English + Italian average) across all Nesso and
         baseline models. Data from Table~\ref{tab:nesso-main}, column \emph{Media
         Totale}.}
\label{fig:nesso-all}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/italian-nesso.png}
\caption{Italian-language benchmark results. Data from Table~\ref{tab:nesso-main}, columns
         IFEval IT, ARC\_IT, HS\_IT, MMLU\_IT.}
\label{fig:italian-nesso}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/english_nesso.png}
\caption{English-language benchmark results. Data from Table~\ref{tab:nesso-main}, columns
         IFEval EN, ARC\_EN, HS\_EN, MMLU\_EN.}
\label{fig:english-nesso}
\end{figure}

\subsection*{Discussion}

As observed, Qwen maintains a clear advantage on MMLU~\cite{hendrycks2021mmlu} (both
English and Italian). However, across several other benchmarks---particularly
IFEval~\cite{zhou2023ifeval} and HellaSwag~\cite{zellers2019hellaswag}---Nesso achieves
competitive or superior performance. Considering that MMLU is a widely used and often
saturated benchmark, frequently incorporated into training corpora, we believe our results
demonstrate that we have created a highly competitive small language model optimized for
English/Italian edge inference scenarios.

% ----------------------------------------------------------------
% Open-Nesso is part of Section 7, not a top-level section (matching README ToC).
% ----------------------------------------------------------------
\subsection{Open-Nesso-0.4B Evaluation}
\label{subsec:open-nesso}
% ----------------------------------------------------------------

Open-Nesso-0.4B-ita is our fully open-source variant. It is based on Nesso-0.4B-ita and
trained on the publicly available dataset published by Michele Montebovi:
\url{https://huggingface.co/datasets/DeepMount00/OpenItalianData}~\cite{openitaliandata}.

Table~\ref{tab:open-nesso} reports Italian-language benchmark scores.

\begin{table}[H]
\caption{Open-Nesso-0.4B evaluation (Italian benchmarks).}
\label{tab:open-nesso}
\centering
\begin{tabular}{lcccc}
\toprule
Model & mmlu\_it & arc\_it & hellaswag\_it & Media \\
\midrule
giux78/Open-Zagreus-0.4B & 0.2530 & 0.3020 & 0.3608 & 0.3053 \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Evalita Comparison with Base Model}
\label{subsubsec:evalita-comparison}

Table~\ref{tab:evalita-comparison} and Figure~\ref{fig:evalita-comparison} compare
Open-Zagreus-0.4B against the base Zagreus-0.4B-ita on the EVALITA
suite~\cite{evalita2023}. Refer to Table~\ref{tab:evalita-fbk} in
Section~\ref{subsec:evalita-fbk} for the full base-model breakdown.

\subsubsection*{Evaluation Command}
\begin{lstlisting}[language=bash]
lm_eval --model hf \
  --model_args pretrained=meta-llama/Llama-2-7b-hf \
  --tasks evalita-mp --device cuda:0 --batch_size 1
\end{lstlisting}

\begin{table*}[H]
\caption{EVALITA task-level comparison: Zagreus-0.4B-ita-base
         (see Table~\ref{tab:evalita-fbk}) vs.\ Open-Zagreus-0.4B (SFT on
         \cite{openitaliandata}).}
\label{tab:evalita-comparison}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{l l c c c}
\toprule
Task & Metric & Zagreus-0.4B-ita & Open-Zagreus-0.4B & $\Delta$ (SFT -- Base) \\
\midrule
\textbf{Overall}     & acc    & 0.3226 & \textbf{0.3313} & +0.0087 \\
admission-test       & acc    & \textbf{0.2137} & 0.2083 & $-$0.0054 \\
faq                  & acc    & \textbf{0.2681} & 0.2672 & $-$0.0009 \\
hate-speech          & f1     & \textbf{0.6056} & 0.4340 & $-$0.1716 \\
lexical-substitution & f1     & 0.0000 & 0.0000 & $=$ \\
NER                  & f1     & \textbf{0.1611} & 0.1357 & $-$0.0254 \\
relation-extraction  & f1     & \textbf{0.1244} & 0.0000 & $-$0.1244 \\
sentiment            & f1     & 0.3660 & \textbf{0.3712} & +0.0052 \\
summarization        & rouge1 & 0.1947 & \textbf{0.2305} & +0.0358 \\
text-entailment      & acc    & 0.5133 & \textbf{0.5492} & +0.0359 \\
word-in-context      & f1     & 0.4697 & \textbf{0.4880} & +0.0183 \\
\bottomrule
\end{tabular}%
}
\end{table*}

\begin{figure}[H]
\centering
\includegraphics[width=\linewidth]{images/evalita_comparison.png}
\caption{Per-task EVALITA comparison: base model (Zagreus-0.4B-ita) vs.\
         Open-Zagreus-0.4B. Data from Table~\ref{tab:evalita-comparison}.}
\label{fig:evalita-comparison}
\end{figure}

The model and dataset demonstrate that it is possible to build competitive English--Italian
language models using exclusively open-source resources.

% ================================================================
\section{Conclusion}
\label{sec:conclusion}
% ================================================================

The \emph{Zagreus and Nesso Model Families} project stands as a remarkable and highly
important contribution to the field of language model research, particularly within the
realm of \textbf{Small Language Models (SLMs)}. At a time when the community is largely
focused on scaling models ever larger, this work demonstrates that \textbf{starting from
scratch and engineering a small, efficient model can be both feasible and impactful}.

The initiative directly addresses the critical need for models that are capable of
\textbf{intelligent reasoning at the edge}, optimally suited for deployment on everyday
devices with limited compute resources---a paradigm that will only grow in strategic
importance as AI becomes more ubiquitous across hardware platforms.

This report does not merely describe a model; it \textbf{documents the entire empirical
journey} of developing SLMs from first principles, from motivation and data engineering to
pre-training, validation, and deployment. By releasing \textbf{seven distinct models},
including multilingual foundational checkpoints and post-trained variants optimized for
conversational and agentic use cases, the project sets a new standard for reproducibility
and openness in LLM research. Importantly, the work emphasizes that \textbf{carefully
engineered small models can match or approach the performance of much larger counterparts on
standardized benchmarks}~\cite{hendrycks2021mmlu,zellers2019hellaswag,clark2018arc,zhou2023ifeval},
underlining a strategic shift in how the community can think about computational efficiency
without sacrificing capability.

In addition to the scientific and technical achievements, the \emph{Zagreus--Nesso SLM}
effort embodies a broader philosophical commitment: the advancement and
\textbf{democratization of AI through open research and open-source tools}. By providing
detailed data pipelines, architectural choices, and a transparent account of trade-offs
encountered in training at scale, this work becomes an invaluable resource for anyone
seeking to replicate or build upon it. Therefore, the importance of this report lies not
only in its immediate results but also in its \textbf{lasting influence on how future small
language models may be conceived, trained, and deployed}.

% ================================================================
\section*{Declaration on Generative AI}
% ================================================================

During the preparation of this work, the author(s) used generative AI tools for grammar
and spelling refinement. After using these tool(s), the author(s) reviewed and edited the
content as needed and take full responsibility for the publication's content.

\bibliography{references}

\end{document}
